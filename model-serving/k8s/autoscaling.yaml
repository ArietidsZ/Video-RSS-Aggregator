apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: triton-server-hpa
  namespace: model-serving
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: triton-inference-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # GPU utilization (custom metric)
  - type: Pods
    pods:
      metric:
        name: gpu_utilization
      target:
        type: AverageValue
        averageValue: "75"
  # Request latency (custom metric)
  - type: Pods
    pods:
      metric:
        name: inference_request_duration_p99
      target:
        type: AverageValue
        averageValue: "500m"  # 500ms
  # Queue depth (custom metric)
  - type: Pods
    pods:
      metric:
        name: triton_request_queue_size
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      selectPolicy: Max
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      selectPolicy: Min
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 120

---
apiVersion: autoscaling/v1
kind: VerticalPodAutoscaler
metadata:
  name: triton-server-vpa
  namespace: model-serving
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: triton-inference-server
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: triton-server
      minAllowed:
        cpu: "4"
        memory: "16Gi"
        nvidia.com/gpu: "1"
      maxAllowed:
        cpu: "32"
        memory: "128Gi"
        nvidia.com/gpu: "4"
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: triton-server-pdb
  namespace: model-serving
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: triton-server
  unhealthyPodEvictionPolicy: IfHealthyBudget

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaler-config
  namespace: model-serving
data:
  scaling_rules.yaml: |
    rules:
      - name: business_hours_scaling
        schedule: "0 9 * * 1-5"  # 9 AM weekdays
        minReplicas: 5
        maxReplicas: 15
        duration: 10h

      - name: night_scaling
        schedule: "0 22 * * *"  # 10 PM daily
        minReplicas: 2
        maxReplicas: 5
        duration: 8h

      - name: weekend_scaling
        schedule: "0 0 * * 6,0"  # Weekends
        minReplicas: 3
        maxReplicas: 8
        duration: 48h

  metrics_config.yaml: |
    custom_metrics:
      - name: gpu_utilization
        query: |
          avg(rate(container_accelerator_duty_cycle{namespace="model-serving"}[5m])) * 100

      - name: inference_request_duration_p99
        query: |
          histogram_quantile(0.99,
            sum(rate(nv_inference_request_duration_us_bucket{namespace="model-serving"}[5m]))
            by (le)
          ) / 1000

      - name: triton_request_queue_size
        query: |
          sum(nv_inference_pending_request_count{namespace="model-serving"})

      - name: model_memory_usage
        query: |
          sum(nv_gpu_memory_used_bytes{namespace="model-serving"}) / (1024*1024*1024)

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-cache-warmer
  namespace: model-serving
spec:
  schedule: "*/30 * * * *"  # Every 30 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cache-warmer
            image: curlimages/curl:latest
            command:
            - sh
            - -c
            - |
              # Warm up Whisper model
              curl -X POST http://triton-inference-service:8000/v2/models/whisper_turbo/infer \
                -H "Content-Type: application/json" \
                -d '{
                  "inputs": [{
                    "name": "audio",
                    "shape": [1, 16000],
                    "datatype": "FP32",
                    "data": [0.0]
                  }]
                }'

              # Warm up LLM model
              curl -X POST http://triton-inference-service:8000/v2/models/llama3_summarizer/infer \
                -H "Content-Type: application/json" \
                -d '{
                  "inputs": [{
                    "name": "text",
                    "shape": [1],
                    "datatype": "BYTES",
                    "data": ["warmup"]
                  }]
                }'
          restartPolicy: OnFailure