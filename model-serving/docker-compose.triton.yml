version: '3.8'

services:
  # NVIDIA Triton Inference Server
  triton-server:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: triton-server
    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics
    volumes:
      - ./model_repository:/models
      - ./plugins:/plugins
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3
      - LD_PRELOAD=/plugins/libtriton_faster_whisper.so
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --load-model=whisper_turbo
      --load-model=llama3_summarizer
      --load-model=silero_vad
      --load-model=scene_detector
      --strict-model-config=false
      --gpu-metrics-interval=1000
      --metrics-port=8002
      --http-port=8000
      --grpc-port=8001
      --allow-gpu-metrics=true
      --log-verbose=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - model-serving-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Model Registry and Version Control
  model-registry:
    image: postgres:15-alpine
    container_name: model-registry
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_DB=model_registry
      - POSTGRES_USER=triton
      - POSTGRES_PASSWORD=ModelServing2024!
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - model-serving-network

  # MinIO for Model Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=ModelStorage2024!
    command: server /data --console-address ":9001"
    volumes:
      - ./data/minio:/data
    networks:
      - model-serving-network

  # Prometheus for Metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus-triton
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - model-serving-network

  # Grafana for Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana-triton
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=redis-datasource,redis-app
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - model-serving-network

  # Model Optimizer Service
  model-optimizer:
    build: ./model-optimizer
    container_name: model-optimizer
    ports:
      - "8003:8003"
    environment:
      - TRITON_SERVER_URL=triton-server:8001
      - OPTIMIZATION_LEVEL=O3
      - ENABLE_TENSORRT=true
      - ENABLE_QUANTIZATION=true
    volumes:
      - ./model_repository:/models
      - ./optimization_cache:/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    networks:
      - model-serving-network

  # Load Balancer
  nginx-lb:
    image: nginx:alpine
    container_name: nginx-lb
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/etc/nginx/certs
    depends_on:
      - triton-server
    networks:
      - model-serving-network

networks:
  model-serving-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16